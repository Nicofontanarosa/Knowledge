# Libpcap ðŸ“¡

#LibPcap #network #networks #sniffing #wireshark #tcpdump #WinPcap #snort #tshark 

> [!TIP]
> 
> ***Introduction of Packet Capture***
> 
> Da quando le workstation sono diventate interconnesse, gli amministratori di rete hanno sentito lâ€™esigenza di â€œvedereâ€ cosa scorre sui cavi. Abbiamo giÃ  visto, nel file _Packet AnalysisðŸ“¡_, come sia possibile catturare i pacchetti, ma _come funziona davvero questo meccanismo ?_
> 
> I sistemi operativi hanno sviluppato delle **API** per lo sniffing dei pacchetti. Tuttavia, poichÃ© non esisteva un vero e proprio standard, ogni sistema operativo dovette inventare una propria API: [Ultrix Packet Filter di DEC](AA-PBM2A-TE_Ultrix_4.0_The_Packet_Filter_-_An_Efficient_Mechanism_for_User-Level_Network_Code_Jun1990.pdf), [Snoop di Solaris](AB_Snoop) e altre ancora
> 
> Questo ha portato a numerose complicazioni: le API piÃ¹ semplici copiavano tutti i pacchetti nello sniffer nello spazio utente, causando un'enorme quantitÃ  di lavoro inutile su sistemi molto attivi. Le API piÃ¹ complesse, invece, permettevano di filtrare i pacchetti prima di trasferirli allo spazio utente, ma spesso erano macchinose e lente
>  
> Tutto cambiÃ² nel 1993, quando Steven #McCanne e Van #Jacobson pubblicarono un articolo che introduceva un metodo piÃ¹ efficiente per filtrare i pacchetti direttamente nel kernel. Lo chiamarono ["The BSD Packet Filter" (BPF)](BPF.pdf).
> 
> <p align="center"><img src="img/Screenshot 2025-02-13 180844.png" /></p>
>

> [!IMPORTANT]
> 
> ***What is BPF ?***
> 
> <mark>**BPF - Berkeley Packet Filter**</mark> Ã¨ un paradigma originariamente sviluppato per filtrare i pacchetti di rete a livello kernel
> 
> <p align="center"><img src="img/Screenshot 2025-02-11 165733.png" /></p>
> 
> Normalmente, il traffico Internet segue lo stack a destra, passando attraverso il **driver della scheda di rete ( Ethernet device driver )**, che permette al sistema operativo di interagire con l'hardware di rete.
> 
> Sappiamo che ogni processo sulla macchina vede solo ciÃ² che il sistema operativo gli consente di vedere. Possiamo quindi affermare che ogni processo Ã¨ "chiuso" nel proprio ambiente. **Ma come puÃ² un processo in spazio utente leggere i dati che viaggiano sulla scheda di rete, senza accedere direttamente allo spazio kernel ?**
> 
> In passato si usavano **chiamate di sistema**, che perÃ², pur essendo sicure, risultavano lente. Una soluzione piÃ¹ efficiente Ã¨ usare applicazioni che accedono direttamente alla memoria tramite #DMA ( _Direct Memory Access_ ), permettendo di leggere/scrivere pacchetti bypassando la #CPU e riducendo la latenza
>
> <p align="center"><img src="img/Screenshot 2025-02-11 171841.png" /></p>   
> <p align="center"><img src="img/Screenshot 2025-02-11 174853.png" /></p>
> 
> Questa **memoria circolare** viene creata nel **kernel** e contiene la copia dei pacchetti catturati. Essa Ã¨ **condivisa** ( o mappata ) con lo **spazio utente**, permettendo cosÃ¬ all'applicazione di monitoraggio di accedervi direttamente
> 
> Per migliorare la velocitÃ  di accesso, **non viene usata sincronizzazione con variabili di controllo**, poichÃ© esiste **un solo produttore** ( kernel ) e **un solo consumatore** ( sniffer ). Il buffer Ã¨ gestito con un **indice di testa** ( _write index_ ) e un **indice di coda** ( *read index* ) che **non devono mai coincidere**: devono sempre essere distanziati almeno di una cella. Questo consente allo sniffer di leggere in sicurezza tutti i pacchetti **fino al penultimo inserito**, lasciando sempre una cella vuota per distinguere il buffer pieno da quello vuoto
> 
> Questo tipo di **buffer circolare** consente un accesso molto efficiente ai pacchetti, ma in situazioni di traffico intenso puÃ² comunque verificarsi la perdita di pacchetti ( _kernel packet drops_ )
> 
> Se l'applicazione sniffer Ã¨ **multi-thread**, si presentano nuove sfide -> **piÃ¹ thread** proveranno ad accedere a **un unico buffer** condiviso, causando **contenzione** ( *nic -> buffer condiviso -> buffer applicazione* ). Una prima ottimizzazione consiste nell'introdurre **n buffer separati**, ognuno gestito da un #thread diverso. Un **thread smistatore** si occupa di leggere dal buffer condiviso ( kernel-user ) e distribuire i pacchetti nei buffer locali dei thread sniffer
> 
> Per migliorare ulteriormente, Ã¨ possibile **eliminare il thread smistatore** spostando la logica di smistamento **nel kernel stesso**. In questo caso, il kernel **mappa direttamente n buffer**, uno per ogni thread dell'applicazione utente. CosÃ¬, il kernel smista i pacchetti ricevuti dalla NIC **direttamente nei buffer corrispondenti** usando una politica ( #round_robin )
> 
> Queste ottimizzazioni funzionano bene in scenari **1:1 ( un produttore e un consumatore )**. In configurazioni **1:n** diventano piÃ¹ complesse da gestire
> 
> Questa tecnica prende il nome di <mark>**Receive Side Scaling ( RSS )**</mark> ->
> 
> #RSS distribuisce la ricezione dei pacchetti **tra i vari core** della CPU per evitare il **collo di bottiglia** derivante dall'accesso simultaneo a un'unica struttura dati. Di default, il kernel puÃ² assegnare **un buffer per ogni core**, ma si puÃ² anche configurare un numero maggiore. Infatti, avere **piÃ¹ buffer rispetto ai core** puÃ² essere vantaggioso ( *mettere - buffer significa che + core lavoreranno su un unico buffer avendo problemi di sincronizzazione, metterne di + non Ã¨ una soluzione cosÃ¬ sbagliata invece poichÃ© se un core Ã¨ abbastanza veloce potrebbe lavorare su + buffer* )
> 
> <p align="center"><img src="img/Screenshot 2025-02-11 164123.png" /></p>
> 
> ---
> 
> A livello concettuale, troviamo il **driver BPF** mantiene lâ€™elenco delle **applicazioni registrate** per catturare pacchetti e, per ognuna, gestisce una **coda dedicata** contenente i pacchetti filtrati. Questo significa che sniffare i pacchetti **consuma memoria**, poichÃ© ogni applicazione ha il proprio spazio di buffering
> 
> Il filtraggio viene eseguito **a livello kernel**, anzichÃ© a livello utente, per **evitare spostamenti inutili** di pacchetti nel buffer circolare e **ridurre il numero di pacchetti processati inutilmente**
> 
> Il **filtro definito dallâ€™utente** determina ->
> 
> - **Se un pacchetto deve essere accettato**
> - **Quanti byte del pacchetto devono essere salvati**
>
> Per ogni filtro che accetta il pacchetto, **BPF copia nel buffer associato** solo la quantitÃ  di dati richiesta. Prima dellâ€™applicazione del filtro, viene utilizzato un **puntatore**: il pacchetto viene **filtrato "sul posto"** ( **direttamente nella memoria dove il motore DMA della scheda di rete lo ha scritto** ), evitando di copiarlo in un altro buffer prima di applicare il filtro. Il numero di **reference** del pacchetto viene aggiornato per gestire lâ€™accesso ai dati
> 
> Successivamente, lo **sniffer** legge i pacchetti dal **buffer condiviso** e li trasferisce nel proprio **buffer locale** per lâ€™elaborazione
> 
> Un **filtro di pacchetti** Ã¨ semplicemente una **funzione booleana** applicata a ogni pacchetto
> 
> PiÃ¹ il pacchetto Ã¨ grande, **maggiore sarÃ  la dimensione della cella** nel buffer circolare.
> 
> <p align="center"><img src="img/Screenshot 2025-02-13 185937.png" /></p>
> 
> Il **BPF** ( e la sua evoluzione **eBPF** ) Ã¨ una vera e propria **macchina virtuale** che accetta **programmi ( filtri )** scritti in uno specifico **bytecode**, provenienti dallo **spazio utente**. Questi programmi vengono caricati nel kernel tramite una **chiamata di sistema** â€“ che avviene **una sola volta** â€“ e da quel momento in poi vengono eseguiti direttamente nel kernel, senza ulteriori interventi dallo spazio utente.
> 
> <mark>_**eXpress Data Path ( XDP )**_</mark> Ã¨ un framework che consente l'elaborazione dei pacchetti **ad altissima velocitÃ **, integrando programmi BPF direttamente **all'ingresso dell'interfaccia di rete**. In pratica, un programma BPF puÃ² essere eseguito **appena il pacchetto viene ricevuto**, _prima ancora che venga processato dallo stack TCP/IP del kernel_, permettendo cosÃ¬ azioni immediate come il **drop**, il **redirect** o la **modifica del pacchetto**.
> 
> Un interessante approfondimento sull'ottimizzazione della macchina virtuale BPF per architetture **x86** Ã¨ disponibile in [questo articolo](https://lwn.net/Articles/437981/)
> 

> [!IMPORTANT]
> 
> ***What is TcpDump ?***
>
> **Tcpdump** Ã¨ uno strumento di analisi del traffico di rete basato su riga di comando, composto da tre parti logiche principali =>
>  
> 1. **Parser di espressioni** -> Tcpdump analizza prima un'espressione di filtro leggibile comeÂ `ip e udp e la porta 53`. Il risultato Ã¨ un breve programma in uno speciale bytecode minimo, il bytecode BPF. Il modo piÃ¹ semplice per vedere il parser in azione Ã¨ passare un flagÂ `-d`, che produrrÃ  un programma leggibile simile a un assembly ->
>
> <p align="center"><img src="img/Screenshot 2025-02-13 183429.png" /></p>
> 
> Questo programma si legge cosÃ¬ ->
> - Caricare una mezza parola (2 byte) dal pacchetto all'offset 12
> - Controlla se il valore Ã¨ 0x0800, altrimenti fallisci. In questo modo viene verificata la presenza del pacchetto IP suÂ un frame Ethernet
> - ...
> 
> Qui Ã¨ possibile trovareÂ [la documentazione completa della sintassi dei filtri](https://www.kernel.org/doc/Documentation/networking/filter.txt)
> ed alcuni [esempi](https://unix.stackexchange.com/questions/699500/understanding-of-bpf)
> 
> <mark>**The biggest user of this construct ( BPF ) might be libpcap**</mark>. TcpDump quando viene eseguito passa attraverso il compilatore interno di questa libreria che genera una struttura che viene passata al kernel e usata come filtro. Con il flag  `-ddd` mostriamo cosa viene caricato all'interno di questa struttura
>
>  1. Il bytecode BPF Ã¨ collegato all'interfaccia di rete #TAP
>  2. TcpDumpo stampa i pacchetti per cui il filtro Ã¨ soddisfatto
>
> In sostanza, Tcpdump chiede al kernel di eseguire un programma BPF all'interno del contesto del kernel. Potrebbe sembrare rischioso, ma in realtÃ  non lo Ã¨. Prima di eseguire il bytecode BPF, il kernel si assicura che sia sicuro ->
> 
> 1. Tutti i salti sono solo in avanti, il che garantisce che non ci siano loop nel programma BPF. Pertanto deve terminare
> 2. Tutte le istruzioni, in particolare le letture in memoria, sono valide e nel raggio d'azione
> 3. Il singolo programma BPF ha meno di 4096 istruzioni
>

> [!NOTE]
> 
> ***Usage of TcpDump***
> 
> ***Promisque*** ci consente di vedere effettivamente il traffico internet che non viene dalla nostra scheda di rete, poichÃ© le schede di rete fanno il filtraggio del MAC address
> 
> `sudo tcpdump -i any`
> 
> <p align="center"><img src="img/Screenshot 2025-02-13 191834.png" /></p>
> 
> Qui Ã¨ possibile trovare un ottima [guida](https://andreaskaris.github.io/blog/networking/bpf-and-tcpdump/)
> 

> [!TIP]
> 
> ***What is Libpcap ?***
>
>  LaÂ **libreria** libpcap <mark>**Packet capture library**</mark>Â Ã¨ stata scritta come **parte di un programma** piÃ¹ ampio chiamato <mark>***TCPDump***</mark>. La libreria libpcap ha permesso agli sviluppatori di scrivere codice per ricevere pacchetti di livello di collegamento ( *livello 2 nel modello OSI* ) su diversi tipi di **sistemi operativi UNIX** senza doversi preoccupare delle differenze sulle varie schede di rete e dei driver dei diversi sistemi operativi. In sostanza, la libreria libpcap **cattura i pacchetti direttamente dalle schede di rete**, il che ha permesso agli sviluppatori di scrivere programmi per decodificare, visualizzare o registrare i pacchetti
>  
>  Per maggiori informazioni attenersi all'APIÂ [libpcapÂ di alto livello](http://www.tcpdump.org/manpages/pcap.3pcap.html)
>
> Questa libreria Ã¨ usata frequentemente negli strumenti di sicurezza di rete per una varietÃ  di scopi, inclusi scanner di rete e software di monitoraggio di rete. Mentre molte piattaforme UNIX sono fornite di default conÂ _libpcap_Â , **la piattaforma Windows** non lo Ã¨ ma Ã¨ fornita di un driver di pacchettiÂ <mark>**WinPcap**</mark>  
>  
>  Il programma **TCPDump** ha fatto proprio questo. **Uno sniffer multipiattaforma**, originariamente scritto da #Van_Jacobson, #Craig_Leres e #Steven_McCanne presso i #Lawrence #Berkeley Labs per **analizzare i problemi di prestazioni TCP**, TCPDump ha permesso di catturare pacchetti e quindi decodificarli e visualizzarli. Un giorno, frustrato dalle limitazioni e dai formati di output di TCPDump, #Marty_Roesch ha scritto <mark>**Snort**</mark> come sostituto di TCPDump ( *era semplicemente un TCPDump migliore* )
>

> [!IMPORTANT]
> 
> ***How Wireshark Work?***
> 
> Sulla maggior parte delle moderne piattaforme UNIX Ã¨ disponibile libpcap. E' installato di default su BSD e macOS, e potrebbe essere installato di default anche sulle distribuzioni Linux
> 
> Sono disponibili due versioni Windows di libpcap. Il piÃ¹ vecchio si chiamaÂ [WinPcap](https://wiki.wireshark.org/WinPcap); Non viene piÃ¹ mantenuto attivamente ed Ã¨ basato su una versione precedente di libpcap. Il piÃ¹ recente si chiamaÂ [Npcap](https://nmap.org/npcap/); Ã¨ attivamente mantenuto ed Ã¨ basato su una versione relativamente recente di libpcap ed Ã¨ disponibile solo per Windows 7 e versioni successive di Windows
> 
> La versione di Wireshark su terminal Ã¨ chiamata <mark>**tshark**</mark>
> 
> In Wireshark, utilizzando il linguaggio Lua, esistono tre meccanismi principali per ispezionare ed elaborare i pacchetti =>
> 
> - **Dissector** -> Utilizzato quando si vuole modificare il modo in cui Wireshark mostra il pacchetto, analizzando il payload grezzo di un protocollo. Viene chiamato durante la fase di dissezione, ovvero appena Wireshark identifica il protocollo
> - **Post-dissector** -> Usato quando si vogliono analizzare i pacchetti dopo che tutti i dissector standard hanno terminato la loro analisi, per aggiungere informazioni extra o confrontare dati tra piÃ¹ protocolli. Non analizza direttamente il payload, ma legge i campi giÃ  estratti da altri dissector
> - **Tap** -> Utile per raccogliere statistiche, dati temporali o esportare flussi di pacchetti
> 
> Wireshark mostra dei **metadati** racchiusi tra parentesi **quadre \[\]**
> 

---